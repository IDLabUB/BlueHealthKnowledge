{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48edd1e3",
   "metadata": {},
   "source": [
    "\n",
    "# Worldwide Research Footprint\n",
    "\n",
    "Map first-author affiliations for the Blue Health literature. We use the stored\n",
    "PubMed metadata to infer the country of the first author for each article and\n",
    "summarize publication hotspots on a world map.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59a1cb6",
   "metadata": {},
   "source": [
    "\n",
    "> **Data requirement**: this notebook expects that `scripts/collect_words.py`\n",
    "> has already been executed (for factors and, optionally, exposures) so that\n",
    "> `data/words/words_*.p` files and their raw article metadata are available.\n",
    "> Run the collection step first if those files are missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16de2365",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from lisc.utils import SCDB, load_object\n",
    "\n",
    "\n",
    "def ensure_package(package: str, import_name: str | None = None):\n",
    "    \"\"\"Import a package, installing it on-demand if needed.\"\"\"\n",
    "    name = import_name or package\n",
    "    try:\n",
    "        return importlib.import_module(name)\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package} â€¦\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        except Exception as exc:\n",
    "            raise RuntimeError(\n",
    "                f\"Failed to install {package}. Install it manually and rerun.\"\n",
    "            ) from exc\n",
    "        return importlib.import_module(name)\n",
    "\n",
    "\n",
    "px = ensure_package(\"plotly\", \"plotly.express\")\n",
    "pycountry = ensure_package(\"pycountry\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaf0662",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Optional, Tuple\n",
    "import pickle\n",
    "\n",
    "\n",
    "def find_repo_root(marker: str = \"requirements.txt\") -> Path:\n",
    "    \"\"\"Ascend directories until the repository root is found.\"\"\"\n",
    "    path = Path.cwd().resolve()\n",
    "    while path != path.parent:\n",
    "        if (path / marker).exists():\n",
    "            return path\n",
    "        path = path.parent\n",
    "    raise RuntimeError(\"Could not locate the repository root from the current directory.\")\n",
    "\n",
    "\n",
    "REPO_ROOT = find_repo_root()\n",
    "WORDS_DIR = REPO_ROOT / \"data\" / \"words\"\n",
    "WORDS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "available_paths = sorted(WORDS_DIR.glob(\"words_*.p\"))\n",
    "if not available_paths:\n",
    "    raise FileNotFoundError(\n",
    "        \"No words_*.p files were found under data/words/. \"\n",
    "        \"Run scripts/collect_words.py (and optionally set BLUEHEALTH_TEST_MODE=1 \"\n",
    "        \"for a quick offline placeholder) before executing this notebook.\"\n",
    "    )\n",
    "\n",
    "print(\"Repository root:\", REPO_ROOT)\n",
    "print(\"Available words pickles:\", [p.name for p in available_paths])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9d765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from lisc import Words\n",
    "\n",
    "\n",
    "def load_words_object(path: Path) -> Words:\n",
    "    \"\"\"Load a Words object from a pickle, trying SCDB fallbacks.\"\"\"\n",
    "    try:\n",
    "        db = SCDB(str(REPO_ROOT))\n",
    "        words_obj = load_object(path.stem, directory=db, reload_results=True)\n",
    "        print(f\"Loaded {path.name} via SCDB.\")\n",
    "        return words_obj\n",
    "    except Exception as exc_scdb:\n",
    "        print(f\"SCDB load failed for {path.name}: {exc_scdb}\")\n",
    "        try:\n",
    "            words_obj = load_object(path.stem, directory=str(path.parent), reload_results=True)\n",
    "            print(f\"Loaded {path.name} from directory path.\")\n",
    "            return words_obj\n",
    "        except Exception as exc_dir:\n",
    "            print(f\"Direct load failed for {path.name}: {exc_dir}\")\n",
    "            with open(path, \"rb\") as handle:\n",
    "                words_obj = pickle.load(handle)\n",
    "            print(f\"Loaded {path.name} via raw pickle (reload_results=False).\")\n",
    "            return words_obj\n",
    "\n",
    "\n",
    "words_objects: list[tuple[str, Words]] = []\n",
    "for path in available_paths:\n",
    "    try:\n",
    "        words_objects.append((path.stem.replace(\"words_\", \"\"), load_words_object(path)))\n",
    "    except Exception as exc:\n",
    "        print(f\"[warn] Skipping {path.name}: {exc}\")\n",
    "\n",
    "if not words_objects:\n",
    "    raise RuntimeError(\"None of the words_*.p files could be loaded.\")\n",
    "\n",
    "print(f\"Loaded {len(words_objects)} words object(s).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936e9e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from lisc.data.articles import Articles\n",
    "\n",
    "\n",
    "def extract_first_author(author_entry):\n",
    "    \"\"\"Return a (name, affiliation) pair for a first-author entry.\"\"\"\n",
    "    if isinstance(author_entry, dict):\n",
    "        name_parts = [author_entry.get(key, \"\") for key in (\"last\", \"first\")]\n",
    "        name = \", \".join(part for part in name_parts if part).strip(\", \") or None\n",
    "        affiliation = author_entry.get(\"affiliation\")\n",
    "    elif isinstance(author_entry, (list, tuple)):\n",
    "        last = str(author_entry[0]).strip() if len(author_entry) > 0 else \"\"\n",
    "        first = str(author_entry[1]).strip() if len(author_entry) > 1 else \"\"\n",
    "        name = \", \".join(part for part in (last, first) if part).strip(\", \") or None\n",
    "        affiliation = author_entry[3] if len(author_entry) > 3 else None\n",
    "    else:\n",
    "        name = None\n",
    "        affiliation = None\n",
    "    if affiliation:\n",
    "        affiliation = affiliation.strip() or None\n",
    "    return name, affiliation\n",
    "\n",
    "\n",
    "def iter_article_records(words_label: str, words_obj: Words):\n",
    "    \"\"\"Yield dictionaries describing each article gathered for a words object.\"\"\"\n",
    "    term_labels = list(getattr(words_obj.terms['A'], 'labels', []))\n",
    "    results = list(getattr(words_obj, 'results', []))\n",
    "    if not results:\n",
    "        return\n",
    "    for term_label, articles in zip(term_labels, results):\n",
    "        n_articles = getattr(articles, 'n_articles', 0)\n",
    "        if not n_articles:\n",
    "            continue\n",
    "        for article in articles:\n",
    "            pmid = article.get('id')\n",
    "            year = article.get('year')\n",
    "            title = article.get('title')\n",
    "            authors = article.get('authors') or []\n",
    "            if not authors:\n",
    "                continue\n",
    "            first_name, affiliation = extract_first_author(authors[0])\n",
    "            yield {\n",
    "                'words_label': words_label,\n",
    "                'term_label': term_label,\n",
    "                'pmid': str(pmid) if pmid is not None else None,\n",
    "                'year': int(year) if isinstance(year, (int, float)) and not np.isnan(year) else year,\n",
    "                'title': title,\n",
    "                'first_author': first_name,\n",
    "                'affiliation': affiliation,\n",
    "            }\n",
    "\n",
    "\n",
    "records = []\n",
    "for words_label, words_obj in words_objects:\n",
    "    for record in iter_article_records(words_label, words_obj):\n",
    "        records.append(record)\n",
    "\n",
    "print(f\"Collected {len(records)} article-level records from the loaded words objects.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00ab019",
   "metadata": {},
   "source": [
    "\n",
    "## Parse first-author affiliations\n",
    "\n",
    "Affiliations often include department, institution, city, and country. We scan the\n",
    "strings from the end, using `pycountry` (plus a few aliases) to infer the country\n",
    "for each first author.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896e0ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ALIAS_MAP = {\n",
    "    'england': 'GBR',\n",
    "    'scotland': 'GBR',\n",
    "    'wales': 'GBR',\n",
    "    'northern ireland': 'GBR',\n",
    "    'united kingdom': 'GBR',\n",
    "    'u.k': 'GBR',\n",
    "    'u.k.': 'GBR',\n",
    "    'uk': 'GBR',\n",
    "    'great britain': 'GBR',\n",
    "    'the netherlands': 'NLD',\n",
    "    'netherlands': 'NLD',\n",
    "    'czech republic': 'CZE',\n",
    "    'south korea': 'KOR',\n",
    "    'republic of korea': 'KOR',\n",
    "    'korea': 'KOR',\n",
    "    'north korea': 'PRK',\n",
    "    'peoples republic of china': 'CHN',\n",
    "    'p.r. china': 'CHN',\n",
    "    'pr china': 'CHN',\n",
    "    'u.s.a': 'USA',\n",
    "    'u.s.': 'USA',\n",
    "    'u.s': 'USA',\n",
    "    'usa': 'USA',\n",
    "    'us': 'USA',\n",
    "    'united states': 'USA',\n",
    "    'united states of america': 'USA',\n",
    "    'ivory coast': 'CIV',\n",
    "    'cote divoire': 'CIV',\n",
    "    'democratic republic of the congo': 'COD',\n",
    "    'republic of the congo': 'COG',\n",
    "    'russia': 'RUS',\n",
    "    'syria': 'SYR',\n",
    "    'palestine': 'PSE',\n",
    "    'taiwan': 'TWN',\n",
    "    'hong kong': 'HKG',\n",
    "    'macau': 'MAC',\n",
    "    'vietnam': 'VNM',\n",
    "    'viet nam': 'VNM',\n",
    "    'burma': 'MMR',\n",
    "    'myanmar': 'MMR',\n",
    "    'laos': 'LAO',\n",
    "    'bolivia': 'BOL',\n",
    "    'venezuela': 'VEN',\n",
    "    'iran': 'IRN',\n",
    "    'uae': 'ARE',\n",
    "    'u.a.e.': 'ARE',\n",
    "    'emirates': 'ARE',\n",
    "    'brunei': 'BRN',\n",
    "    'cape verde': 'CPV',\n",
    "    'eswatini': 'SWZ',\n",
    "    'swaziland': 'SWZ',\n",
    "    'timor leste': 'TLS',\n",
    "    'east timor': 'TLS',\n",
    "    'burundi': 'BDI',\n",
    "    'tanzania': 'TZA',\n",
    "    'macedonia': 'MKD',\n",
    "    'kosovo': 'XKX',\n",
    "    'slovakia': 'SVK',\n",
    "    'moldova': 'MDA'\n",
    "}\n",
    "\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    cleaned = re.sub(r\"[^A-Za-z\\s]\", \" \", text).strip().lower()\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def match_country(candidate: str) -> Optional[Tuple[str, str]]:\n",
    "    \"\"\"Return (ISO3, country name) for a text candidate.\"\"\"\n",
    "    if not candidate:\n",
    "        return None\n",
    "    candidate = candidate.strip()\n",
    "    if not candidate:\n",
    "        return None\n",
    "    try:\n",
    "        country = pycountry.countries.lookup(candidate)\n",
    "        return country.alpha_3, country.name\n",
    "    except LookupError:\n",
    "        pass\n",
    "    normalized = normalize(candidate)\n",
    "    if not normalized:\n",
    "        return None\n",
    "    alias_code = ALIAS_MAP.get(normalized)\n",
    "    if alias_code:\n",
    "        country = (\n",
    "            pycountry.countries.get(alpha_3=alias_code)\n",
    "            or pycountry.countries.get(alpha_2=alias_code)\n",
    "        )\n",
    "        label = country.name if country else alias_code\n",
    "        return alias_code, label\n",
    "    try:\n",
    "        country = pycountry.countries.lookup(normalized)\n",
    "        return country.alpha_3, country.name\n",
    "    except LookupError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def infer_country(affiliation: Optional[str]) -> Optional[Tuple[str, str, str]]:\n",
    "    \"\"\"Infer an ISO3 country code and canonical name from an affiliation string.\"\"\"\n",
    "    if not affiliation:\n",
    "        return None\n",
    "    chunks = re.split(r\"[;/]\", affiliation)\n",
    "    parts: list[str] = []\n",
    "    for chunk in chunks:\n",
    "        pieces = [piece.strip() for piece in chunk.split(',') if piece.strip()]\n",
    "        parts.extend(pieces)\n",
    "    for part in reversed(parts):\n",
    "        match = match_country(part)\n",
    "        if match:\n",
    "            iso, country_name = match\n",
    "            return iso, country_name, part.strip()\n",
    "    match = match_country(affiliation)\n",
    "    if match:\n",
    "        iso, country_name = match\n",
    "        return iso, country_name, affiliation.strip()\n",
    "    return None\n",
    "\n",
    "\n",
    "for record in records:\n",
    "    guess = infer_country(record.get('affiliation'))\n",
    "    if guess:\n",
    "        record['country_iso3'], record['country'], record['matched_text'] = guess\n",
    "    else:\n",
    "        record['country_iso3'] = None\n",
    "        record['country'] = None\n",
    "        record['matched_text'] = None\n",
    "\n",
    "print(\"Finished inferring countries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7981f566",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "records_df = pd.DataFrame(records)\n",
    "print(\n",
    "    f\"Records with inferred countries: {(records_df['country_iso3'].notna()).sum()} of {len(records_df)}\"\n",
    ")\n",
    "records_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefd00a4",
   "metadata": {},
   "source": [
    "\n",
    "## Aggregate by country\n",
    "\n",
    "We count unique PubMed IDs per country (based on the inferred first-author\n",
    "location). When the same article appears under multiple terms or words objects,\n",
    "we keep a single occurrence per country.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7e4894",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if records_df.empty or not records_df['country_iso3'].notna().any():\n",
    "    raise RuntimeError(\n",
    "        \"No article-level country information was found. Make sure the words objects include \"\n",
    "        \"raw article metadata with author affiliations.\"\n",
    "    )\n",
    "\n",
    "unique_articles = (\n",
    "    records_df.dropna(subset=['country_iso3'])\n",
    "    .sort_values(['pmid', 'year'], ascending=[True, False])\n",
    "    .drop_duplicates(subset=['pmid'])\n",
    ")\n",
    "\n",
    "country_summary = (\n",
    "    unique_articles.groupby(['country_iso3', 'country'])\n",
    "    .agg(\n",
    "        articles=('pmid', 'nunique'),\n",
    "        first_year=('year', 'min'),\n",
    "        last_year=('year', 'max')\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values('articles', ascending=False)\n",
    ")\n",
    "\n",
    "country_summary.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c566ea",
   "metadata": {},
   "source": [
    "\n",
    "## Visualize the worldwide footprint\n",
    "\n",
    "The choropleth highlights the countries where first authors are publishing blue\n",
    "health research (based on the collected associations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b43c459",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.choropleth(\n",
    "    country_summary,\n",
    "    locations='country_iso3',\n",
    "    color='articles',\n",
    "    hover_name='country',\n",
    "    hover_data={\n",
    "        'articles': True,\n",
    "        'first_year': True,\n",
    "        'last_year': True,\n",
    "        'country_iso3': False,\n",
    "    },\n",
    "    color_continuous_scale='Blues',\n",
    "    title='Blue Health Publications by First Author Country'\n",
    ")\n",
    "fig.update_layout(coloraxis_colorbar={'title': 'Articles'})\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d676d52c",
   "metadata": {},
   "source": [
    "\n",
    "### Top countries and representative terms\n",
    "\n",
    "To provide more context, the table below lists the leading countries along with\n",
    "the blue-health factor labels most frequently associated with their articles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4f758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "country_term = (\n",
    "    records_df.dropna(subset=['country_iso3'])\n",
    "    .groupby(['country', 'term_label'])\n",
    "    .size()\n",
    "    .reset_index(name='article_count')\n",
    ")\n",
    "\n",
    "idx = country_term.groupby('country')['article_count'].idxmax()\n",
    "country_leaders = country_term.loc[idx].sort_values('article_count', ascending=False)\n",
    "\n",
    "summary_with_terms = country_summary.merge(\n",
    "    country_leaders[['country', 'term_label', 'article_count']],\n",
    "    on='country', how='left'\n",
    ")\n",
    "summary_with_terms.rename(\n",
    "    columns={\n",
    "        'term_label': 'leading_term',\n",
    "        'article_count': 'articles_for_leading_term'\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "summary_with_terms.sort_values('articles', ascending=False).head(15)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
